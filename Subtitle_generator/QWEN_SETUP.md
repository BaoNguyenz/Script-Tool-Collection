# Qwen2.5 v·ªõi Ollama - Setup Guide

## ‚ú® ∆Øu ƒëi·ªÉm Qwen2.5:

- ‚úÖ **100% Offline**: Kh√¥ng c·∫ßn API, kh√¥ng c·∫ßn internet
- ‚úÖ **Context-aware**: LLM hi·ªÉuƒë·ªëi tho·∫°i, ch·ªçn ƒë·∫°i t·ª´ ƒë√∫ng
- ‚úÖ **Free unlimited**: Kh√¥ng gi·ªõi h·∫°n
- ‚úÖ **Privacy**: Data kh√¥ng r·ªùi kh·ªèi m√°y b·∫°n
- ‚úÖ **Quality**: G·∫ßn b·∫±ng GPT-4/Gemini cho ti·∫øng Vi·ªát
- ‚úÖ **Fast v·ªõi GPU**: ~3-5s per batch

## üì• B∆∞·ªõc 1: C√†i Ollama

### Windows:

1. Download: **https://ollama.ai/download/windows**
2. Ch·∫°y installer `OllamaSetup.exe`
3. C√†i ƒë·∫∑t theo h∆∞·ªõng d·∫´n
4. Ollama s·∫Ω **t·ª± ƒë·ªông ch·∫°y** sau khi c√†i

### Verify:

M·ªü PowerShell:
```powershell
ollama --version
```

Ph·∫£i hi·ªÉn th·ªã version ‚Üí OK!

## ü§ñ B∆∞·ªõc 2: Pull Qwen2.5 Model

### Option 1: Qwen2.5 7B (Khuy·∫øn ngh·ªã)
**Size**: ~4.7GB  
**VRAM**: ~6GB  
**Speed**: Fast  
**Quality**: R·∫•t t·ªët  

```bash
ollama pull qwen2.5:7b
```

### Option 2: Qwen2.5 14B (Ch·∫•t l∆∞·ª£ng cao h∆°n)
**Size**: ~9GB  
**VRAM**: ~12GB  
**Speed**: Slower  
**Quality**: Xu·∫•t s·∫Øc  

```bash
ollama pull qwen2.5:14b
```

### Option 3: Qwen2.5 3B (Nhanh, VRAM th·∫•p)
**Size**: ~2GB  
**VRAM**: ~3GB  
**Speed**: Very fast  
**Quality**: T·ªët  

```bash
ollama pull qwen2.5:3b
```

**Khuy·∫øn ngh·ªã**: D√πng **7B** - balance t·ªët nh·∫•t!

‚è±Ô∏è **L∆∞u √Ω**: Download m·∫•t 5-15 ph√∫t t√πy t·ªëc ƒë·ªô m·∫°ng.

## ‚öôÔ∏è B∆∞·ªõc 3: Config Model (Optional)

N·∫øu mu·ªën d√πng model kh√°c, edit `translate_vi_qwen.py` d√≤ng 18:

```python
QWEN_MODEL = "qwen2.5:7b"   # Default
# QWEN_MODEL = "qwen2.5:14b"  # Better quality
# QWEN_MODEL = "qwen2.5:3b"   # Faster
```

## üöÄ B∆∞·ªõc 4: Ch·∫°y Script

```bash
python translate_vi_qwen.py
```

Script s·∫Ω:
1. Check Ollama ƒëang ch·∫°y
2. Check model ƒë√£ pull ch∆∞a
3. D·ªãch t·ª´ng batch subtitle v·ªõi context
4. T·∫°o file `*_vi.srt`

## üìä Performance

**Video 1 gi·ªù** (~100 subtitles, 20 batches):

| GPU | Time | Note |
|-----|------|------|
| RTX 4090 | ~1-2 min | Very fast |
| RTX 3060 | ~2-3 min | Fast |
| GTX 1660 | ~3-5 min | Good |
| CPU only | ~10-20 min | Slow but works |

## üîß Troubleshooting

### "Ollama is not running"
```bash
# Check if Ollama service is running
# Windows: Task Manager ‚Üí Services ‚Üí Ollama
# Or restart:
ollama serve
```

### "Model not found"
```bash
# List installed models
ollama list

# Pull missing model
ollama pull qwen2.5:7b
```

### Ch·∫≠m qu√°?
- Th·ª≠ model nh·ªè h∆°n: `qwen2.5:3b`
- Gi·∫£m `BATCH_SIZE` trong script xu·ªëng 3
- Check GPU c√≥ ƒë∆∞·ª£c d√πng kh√¥ng

### Out of memory?
- D√πng model nh·ªè h∆°n: `qwen2.5:3b`
- Close apps kh√°c
- Gi·∫£m `BATCH_SIZE`

## üÜö So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c

| Method | Quality | Pronoun | Offline | Speed | Cost |
|--------|---------|---------|---------|-------|------|
| **Qwen2.5** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚ö°‚ö°‚ö° | Free |
| NLLB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ | ‚ö°‚ö°‚ö° | Free |
| DeepL API | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚ö°‚ö°‚ö° | 500K |
| Gemini API | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚ö°‚ö° | 1M |

## üí° Tips

### C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng:
1. **D√πng model l·ªõn h∆°n** ‚Üí 14B t·ªët h∆°n 7B
2. **Gi·∫£m temperature** ‚Üí Edit script d√≤ng 107: `"temperature": 0.1`
3. **TƒÉng context** ‚Üí Edit `BATCH_SIZE` l√™n 8-10

### TƒÉng t·ªëc ƒë·ªô:
1. **D√πng model nh·ªè** ‚Üí 3B nhanh g·∫•p ƒë√¥i
2. **TƒÉng batch size** ‚Üí √çt API calls h∆°n
3. **Ensure GPU** ‚Üí Check Ollama d√πng GPU

## üìù Model Size vs VRAM

| Model | Model Size | Min VRAM | Ideal VRAM |
|-------|-----------|----------|------------|
| 3B | ~2GB | 3GB | 6GB |
| 7B | ~4.7GB | 6GB | 8GB |
| 14B | ~9GB | 10GB | 12GB |

**N·∫øu kh√¥ng ƒë·ªß VRAM** ‚Üí Ollama t·ª± ƒë·ªông d√πng CPU (ch·∫≠m h∆°n nh∆∞ng v·∫´n work!)

## üéØ Khuy·∫øn ngh·ªã

**Best setup**:
- Model: `qwen2.5:7b`
- GPU: RTX 3060 tr·ªü l√™n (8GB+ VRAM)
- Batch size: 5
- Temperature: 0.3

**Cho s·∫Ω k·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng Gemini/GPT-4 nh∆∞ng ho√†n to√†n OFFLINE!** üöÄ
